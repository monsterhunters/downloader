{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP77GtQS2hO1oYUYEU5BwjT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monsterhunters/downloader/blob/main/ANB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Gdrive Mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3WCj-Iz-zEGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title <img src=\"https://anubisscan.com/wp-content/uploads/2022/05/Anubis-Scan-3.png\" width=\"200px\">\n",
        "\n",
        "\n",
        "import os, json\n",
        "import requests\n",
        "from urllib.parse import urljoin\n",
        "from bs4 import BeautifulSoup\n",
        "from ipywidgets import interact, widgets, HBox\n",
        "from urllib.parse import urlparse\n",
        "from IPython.display import clear_output\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "\n",
        "# Function to extract image links from the script content\n",
        "def extract_image_links(script_content):\n",
        "    if script_content:\n",
        "        # Find the JSON object within the script content\n",
        "        start_index = script_content.find('{')\n",
        "        end_index = script_content.rfind('}') + 1\n",
        "        json_content = script_content[start_index:end_index]\n",
        "\n",
        "        try:\n",
        "            # Load the JSON object\n",
        "            data = json.loads(json_content)\n",
        "\n",
        "            # Extract the image links from the 'images' field\n",
        "            if 'sources' in data:\n",
        "                images = data['sources'][0].get('images', [])\n",
        "                return images\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "    return []\n",
        "\n",
        "download_path = \"\" # @param {type:\"string\"}\n",
        "os.chdir(download_path)\n",
        "\n",
        "\n",
        "# URL of the webpage\n",
        "url = \"\" # @param {type:\"string\"}\n",
        "parsed_url = urlparse(url)\n",
        "base_url = parsed_url.scheme + \"://\" + parsed_url.netloc\n",
        "\n",
        "root = url.split(\"/\")[-2]\n",
        "os.makedirs(root, exist_ok=True)\n",
        "os.chdir(root)\n",
        "\n",
        "response = requests.get(url)\n",
        "html_content = response.text\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "chapter_list_items = soup.find_all('li', {'data-num': True})\n",
        "#print(chapter_list_items)\n",
        "\n",
        "chapter_numbers = []\n",
        "for chapter_item in chapter_list_items:\n",
        "    chapter_link = chapter_item.find('a')['href']\n",
        "    chapter_number_text = chapter_link.split('/')[-2].split('-')[-1]  # Extracting the chapter number from the URL\n",
        "    if chapter_number_text.isdigit():\n",
        "        chapter_numbers.append(int(chapter_number_text))\n",
        "    elif chapter_number_text.upper() == \"END\":\n",
        "        chapter_numbers.append(\"END\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "max_val = max(chapter_numbers)\n",
        "min_val = min(chapter_numbers)\n",
        "\n",
        "# Create a button to stop the download process\n",
        "stop_button = widgets.Button(description=\"Stop Download\")\n",
        "stop_button.layout.width = '20%'\n",
        "\n",
        "# Flag to indicate whether the download process should continue\n",
        "continue_download = True\n",
        "\n",
        "# Define the callback function for the stop button click event\n",
        "def on_stop_button_click(b):\n",
        "    global continue_download\n",
        "    continue_download = False\n",
        "\n",
        "stop_button.on_click(on_stop_button_click)\n",
        "\n",
        "def extract_image_links(script_content):\n",
        "    if script_content:\n",
        "        start_index = script_content.find('{')\n",
        "        end_index = script_content.rfind('}') + 1\n",
        "        json_content = script_content[start_index:end_index]\n",
        "\n",
        "        try:\n",
        "            data = json.loads(json_content)\n",
        "            if 'sources' in data:\n",
        "                images = data['sources'][0].get('images', [])\n",
        "                return images\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "    return []\n",
        "\n",
        "def download_images(image_links, directory):\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    os.chdir(directory)\n",
        "    for link in image_links:\n",
        "       !wget -q --show-progress $link\n",
        "    clear_output()\n",
        "    print('All files have been downloaded!')\n",
        "       #filename = os.path.basename(link)\n",
        "       #response = requests.get(link)\n",
        "       #if response.status_code == 200:\n",
        "       #    with open(filename, 'wb') as f:\n",
        "       #        f.write(response.content)\n",
        "       #        print(f\"Downloaded {filename}\")\n",
        "       #else:\n",
        "       #    print(f\"Failed to download {filename}\")\n",
        "    os.chdir(\"../\")\n",
        "\n",
        "def download_chapters(min_chapter, max_chapter):\n",
        "    global continue_download\n",
        "    continue_download = True\n",
        "\n",
        "    # Iterate over each list item and extract chapter details\n",
        "    for chapter_item in chapter_list_items:\n",
        "        # Check if the download process should continue\n",
        "        if not continue_download:\n",
        "            print(\"Download stopped.\")\n",
        "            return\n",
        "\n",
        "        chapter_number_text = chapter_item.find('span', class_='chapternum').text.strip()\n",
        "        # Check if the chapter number is in the expected format\n",
        "        if not chapter_number_text.startswith('Chapter'):\n",
        "            continue\n",
        "        chapter_number_parts = chapter_number_text.split()\n",
        "        if len(chapter_number_parts) < 2:\n",
        "            continue\n",
        "        chapter_number = chapter_number_parts[1]\n",
        "        if not chapter_number.isdigit():\n",
        "            continue\n",
        "        chapter_number = int(chapter_number)\n",
        "        chapter_date = chapter_item.find('span', class_='chapterdate').text.strip()\n",
        "        chapter_link = chapter_item.find('a')['href']\n",
        "\n",
        "        # Check if the chapter number is within the selected range\n",
        "        if min_chapter <= chapter_number <= max_chapter:\n",
        "            # Construct the complete URL for the chapter page\n",
        "            chapter_url = urljoin(base_url, chapter_link)\n",
        "\n",
        "            # Fetch the HTML content of the chapter page\n",
        "            chapter_response = requests.get(chapter_url)\n",
        "            if chapter_response.status_code == 200:\n",
        "                chapter_html_content = chapter_response.text\n",
        "\n",
        "                # Parse the HTML of the chapter page using BeautifulSoup\n",
        "                chapter_soup = BeautifulSoup(chapter_html_content, 'html.parser')\n",
        "\n",
        "                # Find the script tag containing the desired content\n",
        "                script_tag = chapter_soup.find('script', string=lambda x: 'ts_reader.run' in str(x))\n",
        "                script_content = script_tag.text if script_tag else None\n",
        "\n",
        "                # Extract image links from the script content\n",
        "                image_links = extract_image_links(script_content)\n",
        "                #print(image_links)\n",
        "\n",
        "                # Download images\n",
        "                directory = chapter_url.split(\"/\")[-2]\n",
        "                download_images(image_links, directory)\n",
        "\n",
        "                # Print chapter details\n",
        "                print(f\"Chapter {chapter_number}: {chapter_date} - {chapter_url}\")\n",
        "            else:\n",
        "                print(f\"Failed to fetch chapter page: {chapter_url}\")\n",
        "\n",
        "# Create a slider widget\n",
        "slider = widgets.IntRangeSlider(min=min_val, max=max_val, step=1, value=(min_val, max_val))\n",
        "slider.layout.width = '80%'\n",
        "\n",
        "# Create a download button\n",
        "button = widgets.Button(description=\"Set Chapter\")\n",
        "button.layout.width = '20%'\n",
        "\n",
        "# Define the callback function for the button click event\n",
        "def on_button_click(b):\n",
        "    min_chapter, max_chapter = slider.value\n",
        "    clear_output()\n",
        "    # Open the file in write mode\n",
        "    with open(\"/content/config.txt\", \"w\") as file:\n",
        "        # Write \"Hello, World!\" to the file\n",
        "        file.write(str(download_path)+\"\\n\"+\n",
        "                   str(url)+\"\\n\"+\n",
        "                   str(min_chapter)+\"\\n\"+\n",
        "                   str(max_chapter))\n",
        "    print(\"Chapter \"+str(min_chapter)+\" to \"+str(max_chapter)+\" has been set....\")\n",
        "\n",
        "    print('Run below cell to start download!!!')\n",
        "    #download_chapters(min_chapter, max_chapter)\n",
        "\n",
        "\n",
        "button.on_click(on_button_click)\n",
        "\n",
        "# Display the slider, download button, and stop button\n",
        "print('Set chapters to download')\n",
        "display(slider)\n",
        "display(button)\n",
        "#display(HBox([button, stop_button]))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "OIb82RLcHk0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RUN\n",
        "\n",
        "import os, json\n",
        "import requests\n",
        "from urllib.parse import urljoin\n",
        "from bs4 import BeautifulSoup\n",
        "from ipywidgets import interact, widgets, HBox\n",
        "from urllib.parse import urlparse\n",
        "from IPython.display import clear_output\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "\n",
        "# Function to extract image links from the script content\n",
        "def extract_image_links(script_content):\n",
        "    if script_content:\n",
        "        # Find the JSON object within the script content\n",
        "        start_index = script_content.find('{')\n",
        "        end_index = script_content.rfind('}') + 1\n",
        "        json_content = script_content[start_index:end_index]\n",
        "\n",
        "        try:\n",
        "            # Load the JSON object\n",
        "            data = json.loads(json_content)\n",
        "\n",
        "            # Extract the image links from the 'images' field\n",
        "            if 'sources' in data:\n",
        "                images = data['sources'][0].get('images', [])\n",
        "                return images\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "    return []\n",
        "\n",
        "\n",
        "\n",
        "# Open the file in read mode\n",
        "with open('/content/config.txt', 'r') as file:\n",
        "    # Initialize a counter to keep track of lines read\n",
        "    line_count = 0\n",
        "\n",
        "    # Iterate through each line in the file\n",
        "    for line in file:\n",
        "        # Increment the line count\n",
        "        line_count += 1\n",
        "\n",
        "        # If it's the third line, print it and break the loop\n",
        "        if line_count == 1:\n",
        "          download_path = line.strip()\n",
        "        elif line_count == 2:\n",
        "          url = line.strip()\n",
        "        elif line_count == 3:\n",
        "          min_chapter = int(line.strip())\n",
        "        else:\n",
        "          max_chapter = int(line.strip())\n",
        "\n",
        "\n",
        "\n",
        "#url = \"https://anubisscan.com/manga/ill-eat-your-mom-first/\"\n",
        "#download_path = \"/content/sample_data\"\n",
        "#min_chapter = 2\n",
        "#max_chapter = 3\n",
        "os.chdir(download_path)\n",
        "\n",
        "\n",
        "# URL of the webpage\n",
        "\n",
        "parsed_url = urlparse(url)\n",
        "base_url = parsed_url.scheme + \"://\" + parsed_url.netloc\n",
        "\n",
        "root = url.split(\"/\")[-2]\n",
        "os.makedirs(root, exist_ok=True)\n",
        "os.chdir(root)\n",
        "\n",
        "response = requests.get(url)\n",
        "html_content = response.text\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "chapter_list_items = soup.find_all('li', {'data-num': True})\n",
        "#print(chapter_list_items)\n",
        "\n",
        "chapter_numbers = []\n",
        "for chapter_item in chapter_list_items:\n",
        "    chapter_link = chapter_item.find('a')['href']\n",
        "    chapter_number_text = chapter_link.split('/')[-2].split('-')[-1]  # Extracting the chapter number from the URL\n",
        "    if chapter_number_text.isdigit():\n",
        "        chapter_numbers.append(int(chapter_number_text))\n",
        "    elif chapter_number_text.upper() == \"END\":\n",
        "        chapter_numbers.append(\"END\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Flag to indicate whether the download process should continue\n",
        "continue_download = True\n",
        "\n",
        "def extract_image_links(script_content):\n",
        "    if script_content:\n",
        "        start_index = script_content.find('{')\n",
        "        end_index = script_content.rfind('}') + 1\n",
        "        json_content = script_content[start_index:end_index]\n",
        "\n",
        "        try:\n",
        "            data = json.loads(json_content)\n",
        "            if 'sources' in data:\n",
        "                images = data['sources'][0].get('images', [])\n",
        "                return images\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "    return []\n",
        "\n",
        "def download_images(image_links, directory):\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    os.chdir(directory)\n",
        "    for link in image_links:\n",
        "       !wget -q --show-progress $link\n",
        "    clear_output()\n",
        "    print('All files have been downloaded!')\n",
        "       #filename = os.path.basename(link)\n",
        "       #response = requests.get(link)\n",
        "       #if response.status_code == 200:\n",
        "       #    with open(filename, 'wb') as f:\n",
        "       #        f.write(response.content)\n",
        "       #        print(f\"Downloaded {filename}\")\n",
        "       #else:\n",
        "       #    print(f\"Failed to download {filename}\")\n",
        "    os.chdir(\"../\")\n",
        "\n",
        "def download_chapters(min_chapter, max_chapter):\n",
        "    global continue_download\n",
        "    continue_download = True\n",
        "\n",
        "    # Iterate over each list item and extract chapter details\n",
        "    for chapter_item in chapter_list_items:\n",
        "        # Check if the download process should continue\n",
        "        if not continue_download:\n",
        "            print(\"Download stopped.\")\n",
        "            return\n",
        "\n",
        "        chapter_number_text = chapter_item.find('span', class_='chapternum').text.strip()\n",
        "        # Check if the chapter number is in the expected format\n",
        "        if not chapter_number_text.startswith('Chapter'):\n",
        "            continue\n",
        "        chapter_number_parts = chapter_number_text.split()\n",
        "        if len(chapter_number_parts) < 2:\n",
        "            continue\n",
        "        chapter_number = chapter_number_parts[1]\n",
        "        if not chapter_number.isdigit():\n",
        "            continue\n",
        "        chapter_number = int(chapter_number)\n",
        "        chapter_date = chapter_item.find('span', class_='chapterdate').text.strip()\n",
        "        chapter_link = chapter_item.find('a')['href']\n",
        "\n",
        "        # Check if the chapter number is within the selected range\n",
        "        if min_chapter <= chapter_number <= max_chapter:\n",
        "            # Construct the complete URL for the chapter page\n",
        "            chapter_url = urljoin(base_url, chapter_link)\n",
        "\n",
        "            # Fetch the HTML content of the chapter page\n",
        "            chapter_response = requests.get(chapter_url)\n",
        "            if chapter_response.status_code == 200:\n",
        "                chapter_html_content = chapter_response.text\n",
        "\n",
        "                # Parse the HTML of the chapter page using BeautifulSoup\n",
        "                chapter_soup = BeautifulSoup(chapter_html_content, 'html.parser')\n",
        "\n",
        "                # Find the script tag containing the desired content\n",
        "                script_tag = chapter_soup.find('script', string=lambda x: 'ts_reader.run' in str(x))\n",
        "                script_content = script_tag.text if script_tag else None\n",
        "\n",
        "                # Extract image links from the script content\n",
        "                image_links = extract_image_links(script_content)\n",
        "                #print(image_links)\n",
        "\n",
        "                # Download images\n",
        "                directory = chapter_url.split(\"/\")[-2]\n",
        "                download_images(image_links, directory)\n",
        "\n",
        "                # Print chapter details\n",
        "                print(f\"Chapter {chapter_number}: {chapter_date} - {chapter_url}\")\n",
        "            else:\n",
        "                print(f\"Failed to fetch chapter page: {chapter_url}\")\n",
        "\n",
        "\n",
        "# Define the callback function for the button click event\n",
        "def main(min_chapter, max_chapter):\n",
        "    download_chapters(min_chapter, max_chapter)\n",
        "\n",
        "main(min_chapter, max_chapter)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sN8dsMrBkGmD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}